import numpy as np
import librosa
import sounddevice as sd
import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras.layers import LeakyReLU, Input # Cho custom_objects v√† ƒë·ªãnh nghƒ©a model
import joblib
import json
import os
import tempfile
import pickle

class VoiceAnalyzer:
    def __init__(self, params=None):
        """
        Kh·ªüi t·∫°o ƒë·ªëi t∆∞·ª£ng VoiceAnalyzer v·ªõi c√°c th√¥ng s·ªë v√† m√¥ h√¨nh.
        
        Args:
            params (dict, optional): T·ª´ ƒëi·ªÉn ch·ª©a c√°c th√¥ng s·ªë c·∫•u h√¨nh. 
                                    N·∫øu kh√¥ng cung c·∫•p, s·ª≠ d·ª•ng th√¥ng s·ªë m·∫∑c ƒë·ªãnh.
        """
        # --- 1. KH·ªûI T·∫†O TH√îNG S·ªê ---
        # C√°c th√¥ng s·ªë ch√≠nh (audio v√† m√¥ h√¨nh)
        self.params = params or {
            "SR": 16000,              # T·∫ßn s·ªë l·∫•y m·∫´u
            "N_FFT": 400,             # ƒê·ªô d√†i FFT
            "DEFAULT_OFFSET": 0.0,    # Offset m·∫∑c ƒë·ªãnh khi load file
            "HOP_LENGTH": 160,        # ƒê·ªô d·ªãch khung
            "N_MELS": 40,             # S·ªë bƒÉng t·∫ßn mel
            "MAX_FRAMES": 334,        # S·ªë khung t·ªëi ƒëa
            "MAX_PAD_LEN_SEQ": 334,   # ƒê·ªô d√†i t·ªëi ƒëa sau padding
            "num_expected_coeffs": 60 # S·ªë h·ªá s·ªë ƒë·∫∑c tr∆∞ng
        }
        
        # --- 2. T·∫†O C√ÅC BI·∫æN TH√ÄNH VI√äN T·ª™ PARAMS ---
        # Audio params
        self.sr = self.params["SR"]
        self.n_fft = self.params["N_FFT"] 
        self.hop_length = self.params["HOP_LENGTH"]
        self.n_mels = self.params["N_MELS"]
        self.max_frames = self.params["MAX_FRAMES"]
        self.DEFAULT_OFFSET = self.params["DEFAULT_OFFSET"]
        self.duration_sec = self.max_frames * self.hop_length / self.sr
        
        # C√°c tham s·ªë tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng
        self.INITIAL_LOAD_DURATION = None     # Kh√¥ng gi·ªõi h·∫°n th·ªùi gian
        self.TARGET_SR = self.sr              # T·∫ßn s·ªë m·∫´u ƒë√≠ch
        self.FRAME_LENGTH_MS_SEQ = 25         # ƒê·ªô d√†i khung (ms)
        self.HOP_LENGTH_MS_SEQ = 10           # ƒê·ªô d·ªãch khung (ms) 
        self.N_MFCC_PARAM_SEQ = 20            # S·ªë h·ªá s·ªë MFCC
        self.USE_DELTA_MFCC = True            # S·ª≠ d·ª•ng ƒë·∫°o h√†m b·∫≠c 1
        self.USE_DELTA2_MFCC = True           # S·ª≠ d·ª•ng ƒë·∫°o h√†m b·∫≠c 2
        self.MAX_PAD_LEN_SEQ = self.params["MAX_PAD_LEN_SEQ"]  # S·ª≠ d·ª•ng th√¥ng s·ªë c√≥ s·∫µn

        # --- 3. ƒê∆Ø·ªúNG D·∫™N FILE ---
        self.MODEL_ASSETS_DIR = "model_assets"
        self.MODEL_DIR ="models"
        self.MODEL_CHECKPOINT_PATH = os.path.join(self.MODEL_DIR, "best_crnn_model.keras")
        self.LABEL_ENCODER_PATH = os.path.join(self.MODEL_ASSETS_DIR, "label_encoder_final.pkl")
        self.SCALERS_LIST_PATH = os.path.join(self.MODEL_ASSETS_DIR, "scalers_list.pkl")

        # --- 4. BI·∫æN CHO M√î H√åNH ---
        self.model_loaded = None
        self.le_loaded = None
        self.scalers_list_loaded = None
        self.class_names_loaded = None
        self.num_classes_loaded = 0
        self.input_shape_loaded = (
            self.params["MAX_PAD_LEN_SEQ"],
            self.params["num_expected_coeffs"]
        )

        # --- 5. T·∫¢I M√î H√åNH V√Ä D·ªÆ LI·ªÜU ---
        self._load_model_assets()
    
    def _load_model_assets(self):
        """
        T·∫£i c√°c t√†i nguy√™n m√¥ h√¨nh: LabelEncoder, Scalers v√† m√¥ h√¨nh d·ª± ƒëo√°n.
        X·ª≠ l√Ω c√°c l·ªói n·∫øu c√≥ v√† in th√¥ng b√°o t∆∞∆°ng ·ª©ng.
        """
        try:
            # Ki·ªÉm tra th∆∞ m·ª•c assets
            if not os.path.exists(self.MODEL_ASSETS_DIR):
                raise FileNotFoundError(f"Th∆∞ m·ª•c assets '{self.MODEL_ASSETS_DIR}' kh√¥ng t·ªìn t·∫°i.")
            
            # T·∫£i tu·∫ßn t·ª± c√°c th√†nh ph·∫ßn
            self._load_label_encoder()
            self._load_scalers()
            self._load_model()
            
        except Exception as e:
            print(f"‚õî L·ªñI KH·ªûI T·∫†O: {e}")
    
    def _load_label_encoder(self):
        """T·∫£i v√† kh·ªüi t·∫°o LabelEncoder."""
        if not os.path.exists(self.LABEL_ENCODER_PATH):
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file LabelEncoder: {self.LABEL_ENCODER_PATH}")
            
        with open(self.LABEL_ENCODER_PATH, 'rb') as f:
            self.le_loaded = pickle.load(f)
        
        self.class_names_loaded = list(self.le_loaded.classes_)
        self.num_classes_loaded = len(self.class_names_loaded)
        print(f"‚úÖ ƒê√£ t·∫£i LabelEncoder: {self.class_names_loaded}")
    
    def _load_scalers(self):
        """T·∫£i v√† ki·ªÉm tra scalers."""
        if not os.path.exists(self.SCALERS_LIST_PATH):
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file scaler: {self.SCALERS_LIST_PATH}")
            
        with open(self.SCALERS_LIST_PATH, 'rb') as f:
            self.scalers_list_loaded = pickle.load(f)
        
        print(f"‚úÖ ƒê√£ t·∫£i {len(self.scalers_list_loaded)} scalers.")
        
        # Ki·ªÉm tra t√≠nh nh·∫•t qu√°n
        if len(self.scalers_list_loaded) != self.input_shape_loaded[1]:
            raise ValueError(f"S·ªë scaler kh√¥ng kh·ªõp: {len(self.scalers_list_loaded)} vs {self.input_shape_loaded[1]}")
    
    def _load_model(self):
        """T·∫£i m√¥ h√¨nh Keras v√† c·∫•u h√¨nh."""
        if not os.path.exists(self.MODEL_CHECKPOINT_PATH):
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y m√¥ h√¨nh: {self.MODEL_CHECKPOINT_PATH}")
            
        custom_objects = {'LeakyReLU': LeakyReLU}
        self.model_loaded = load_model(
            self.MODEL_CHECKPOINT_PATH,
            custom_objects=custom_objects,
            compile=False
        )
        print("‚úÖ ƒê√£ t·∫£i m√¥ h√¨nh th√†nh c√¥ng.")
        self.model_loaded.summary()


    def record_audio(self,duration=3.34, sample_rate=16000, filename="tmp.wav"):
        """
        Ghi √¢m gi·ªçng n√≥i t·ª´ micro, l∆∞u ra file WAV, v√† tr·∫£ v·ªÅ:
            - ƒê∆∞·ªùng d·∫´n t·ªõi file WAV ƒë√£ ghi.
            - D·ªØ li·ªáu audio d·∫°ng numpy array (mono, float32).
        
        Parameters:
            duration (int or float): Th·ªùi gian ghi √¢m (gi√¢y).
            sample_rate (int): T·∫ßn s·ªë m·∫´u (Hz).
            filename (str): T√™n file WAV (n·∫øu mu·ªën t·ª± ƒë·∫∑t). N·∫øu kh√¥ng, t·∫°o file t·∫°m.

        Returns:
            (wav_path: str, audio: np.ndarray)
        """
        print(f"üéôÔ∏è ƒêang ghi √¢m trong {duration:.2f} gi√¢y...")
        recording = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1, dtype='float32')
        sd.wait()
        print("‚úÖ Ghi √¢m xong.")

        audio = recording.flatten()

        # X√°c ƒë·ªãnh n∆°i l∆∞u file WAV
        if filename is None:
            tmp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
            filename = tmp_file.name

        # L∆∞u file WAV
        from scipy.io.wavfile import write
        write(filename, sample_rate, recording)
        print(f"üìÅ File WAV ƒë√£ l∆∞u t·∫°i: {filename}")

        return filename, audio
    #H√ÄM TR√çCH XU·∫§T ƒê·∫∂C TR∆ØNG
    def extract_features_for_gradio(self, data, sr,
                                n_fft_val, hop_length_val,
                                n_mfcc_val=None,
                                use_delta=None, use_delta2=None):
        # ƒê·∫∑t gi√° tr·ªã m·∫∑c ƒë·ªãnh n·∫øu ch∆∞a truy·ªÅn v√†o
        if n_mfcc_val is None:
            n_mfcc_val = self.N_MFCC_PARAM_SEQ
        if use_delta is None:
            use_delta = self.USE_DELTA_MFCC
        if use_delta2 is None:
            use_delta2 = self.USE_DELTA2_MFCC
        # (Copy n·ªôi dung h√†m extract_features_sequential t·ª´ B∆∞·ªõc 4.1 v√†o ƒë√¢y)
        features_list = []
        mfccs = librosa.feature.mfcc(y=data, sr=sr, n_mfcc=n_mfcc_val,
                                    n_fft=n_fft_val, hop_length=hop_length_val)
        features_list.append(mfccs)
        if use_delta:
            delta_mfccs = librosa.feature.delta(mfccs)
            features_list.append(delta_mfccs)
        if use_delta2:
            delta2_mfccs = librosa.feature.delta(mfccs, order=2)
            features_list.append(delta2_mfccs)
        combined_features = np.vstack(features_list)
        return combined_features.T

    # --- 6. H√ÄM TI·ªÄN X·ª¨ L√ù CHO M·ªòT FILE √ÇM THANH ƒê·∫¶U V√ÄO ---
    def preprocess_single_audio_file(self,audio_filepath):
        if audio_filepath is None:
            return None, "L·ªói: Kh√¥ng c√≥ file √¢m thanh n√†o ƒë∆∞·ª£c cung c·∫•p."

        try:
            # T·∫£i file √¢m thanh (audio_filepath s·∫Ω l√† ƒë∆∞·ªùng d·∫´n t·∫°m th·ªùi c·ªßa file upload)
            data_orig, sr_orig = librosa.load(audio_filepath, sr=None, offset=self.DEFAULT_OFFSET, duration=self.INITIAL_LOAD_DURATION)
            
            if len(data_orig) == 0: return None, "L·ªói: File √¢m thanh r·ªóng ho·∫∑c kh√¥ng ƒë·ªçc ƒë∆∞·ª£c."

            # Resample
            if sr_orig != self.TARGET_SR:
                data_resampled = librosa.resample(y=data_orig, orig_sr=sr_orig, target_sr=self.TARGET_SR)
            else:
                data_resampled = data_orig
            current_sr = self.TARGET_SR
            if len(data_resampled) == 0: return None, "L·ªói: D·ªØ li·ªáu r·ªóng sau khi resample."

            # T√≠nh to√°n frame v√† hop samples
            frame_samples = int(current_sr * self.FRAME_LENGTH_MS_SEQ / 1000)
            hop_samples = int(current_sr * self.HOP_LENGTH_MS_SEQ / 1000)
            
            if len(data_resampled) < frame_samples:
                return None, f"L·ªói: File √¢m thanh qu√° ng·∫Øn ({len(data_resampled)} m·∫´u) so v·ªõi ƒë·ªô d√†i khung ({frame_samples} m·∫´u)."

            # Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng tu·∫ßn t·ª±
            feature_matrix_raw = self.extract_features_for_gradio(
                data_resampled, current_sr,
                n_fft_val=frame_samples, hop_length_val=hop_samples
            )
            if feature_matrix_raw is None or feature_matrix_raw.size == 0 :
                return None, "L·ªói: Kh√¥ng tr√≠ch xu·∫•t ƒë∆∞·ª£c ƒë·∫∑c tr∆∞ng."


            # Padding / Truncating
            if feature_matrix_raw.shape[0] > self.MAX_PAD_LEN_SEQ:
                feature_matrix_padded = feature_matrix_raw[:self.MAX_PAD_LEN_SEQ, :]
            else:
                pad_width = self.MAX_PAD_LEN_SEQ - feature_matrix_raw.shape[0]
                feature_matrix_padded = np.pad(feature_matrix_raw, pad_width=((0, pad_width), (0, 0)), mode='constant', constant_values=(0,))
            
            if feature_matrix_padded.shape[0] != self.MAX_PAD_LEN_SEQ:
                return None, "L·ªói: K√≠ch th∆∞·ªõc kh√¥ng ƒë√∫ng sau padding."

            # Scaling (s·ª≠ d·ª•ng scalers_list_loaded)
            if self.scalers_list_loaded is None or len(self.scalers_list_loaded) != feature_matrix_padded.shape[1]:
                return None, "L·ªói: Scalers kh√¥ng ƒë∆∞·ª£c t·∫£i ho·∫∑c kh√¥ng kh·ªõp s·ªë l∆∞·ª£ng ƒë·∫∑c tr∆∞ng."
            
            feature_matrix_scaled = np.copy(feature_matrix_padded).astype(np.float32) # ƒê·∫£m b·∫£o float32
            for i in range(feature_matrix_padded.shape[1]): # L·∫∑p qua t·ª´ng c·ªôt feature
                feature_column_flat = feature_matrix_padded[:, i].reshape(-1, 1)
                feature_matrix_scaled[:, i] = self.scalers_list_loaded[i].transform(feature_column_flat).flatten()
                
            # Reshape cho ƒë·∫ßu v√†o m√¥ h√¨nh: (1, timesteps, features)
            final_features = np.expand_dims(feature_matrix_scaled, axis=0)
            return final_features, None # Kh√¥ng c√≥ l·ªói

        except Exception as e:
            return None, f"L·ªói trong qu√° tr√¨nh ti·ªÅn x·ª≠ l√Ω file: {str(e)}"

    def predict_emotion(self,audio_file_path):
        if self.model_loaded is None or self.le_loaded is None:
            return {"L·ªói": "M√¥ h√¨nh ho·∫∑c LabelEncoder ch∆∞a ƒë∆∞·ª£c t·∫£i."}

        # audio_file_path l√† m·ªôt ƒë·ªëi t∆∞·ª£ng file t·∫°m th·ªùi t·ª´ Gradio File/Audio input
        # N√≥ c√≥ thu·ªôc t√≠nh .name ch·ª©a ƒë∆∞·ªùng d·∫´n th·ª±c s·ª± ƒë·∫øn file t·∫°m ƒë√≥
        actual_path = audio_file_path if isinstance(audio_file_path, str) else audio_file_path.name
        
        print(f"ƒêang x·ª≠ l√Ω file: {actual_path}")
        
        features_scaled, error_message = self.preprocess_single_audio_file(actual_path)

        if error_message:
            print(error_message)
            # Tr·∫£ v·ªÅ l·ªói d∆∞·ªõi d·∫°ng dictionary ƒë·ªÉ Gradio hi·ªÉn th·ªã label
            return {error_message.split(':')[0]: 1.0} if ':' in error_message else {"L·ªói x·ª≠ l√Ω": 1.0}


        if features_scaled is not None and features_scaled.ndim == 3 and features_scaled.shape[1:] == self.input_shape_loaded:
            print(f"ƒê·∫∑c tr∆∞ng ƒë√£ x·ª≠ l√Ω c√≥ shape: {features_scaled.shape}")
            probabilities = self.model_loaded.predict(features_scaled)[0] # L·∫•y k·∫øt qu·∫£ cho m·∫´u ƒë·∫ßu ti√™n (v√† duy nh·∫•t)
            
            # Chuy·ªÉn x√°c su·∫•t th√†nh dictionary {label: probability}
            results = {label: float(prob) for label, prob in zip(self.le_loaded.classes_, probabilities)}
            
            # In ra console ƒë·ªÉ debug
            print("X√°c su·∫•t d·ª± ƒëo√°n:")
            for label, prob in results.items():
                print(f"  {label}: {prob:.4f}")
            
            return results
        else:
            print("L·ªói: ƒê·∫∑c tr∆∞ng cu·ªëi c√πng kh√¥ng h·ª£p l·ªá ho·∫∑c shape kh√¥ng ƒë√∫ng.")
            return {"L·ªói ƒë·∫∑c tr∆∞ng": 1.0}

